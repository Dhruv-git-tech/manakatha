import random, json, torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from indicnlp.normalize.indic_normalize import IndicNormalizer
from emoji import emojize

MODEL_NAME = "ai4bharat/indic-bert"          # multilingual BERT
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME, num_labels=3
)  # 0=negative, 1=neutral, 2=positive

normalizer = IndicNormalizer(lang="te")

COMPLIMENTS = [
    "à°…à°¦à±à°­à±à°¤à°®à±ˆà°¨ à°•à°¥! à°¹à±ƒà°¦à°¯à°¾à°¨à±à°¨à°¿ à°¤à°¾à°•à°¿à°‚à°¦à°¿.",
    "à°®à±€à°°à± à°šà±†à°ªà±à°ªà°¿à°¨ à°µà°¿à°·à°¯à°‚ à°šà°¾à°²à°¾ à°¬à°¾à°—à±à°‚à°¦à°¿.",
    "à°…à°¦à±à°­à±à°¤à°‚! à°­à°¾à°µà±‹à°¦à±à°µà±‡à°—à°¾à°²à± à°…à°¦à±à°­à±à°¤à°‚à°—à°¾ à°µà±à°¯à°•à±à°¤à°®à°¯à±à°¯à°¾à°¯à°¿.",
    "à°®à±€ à°…à°¨à±à°­à°µà°‚ à°…à°‚à°¦à°°à°¿à°¨à±€ à°ªà±à°°à°­à°¾à°µà°¿à°¤à°‚ à°šà±‡à°¸à±à°¤à±à°‚à°¦à°¿.",
    "à°šà°¾à°²à°¾ à°¹à±ƒà°¦à±à°¯à°‚à°—à°¾ à°‰à°‚à°¦à°¿ â€“ à°§à°¨à±à°¯à°µà°¾à°¦à°¾à°²à± à°ªà°‚à°šà±à°•à±à°¨à±à°¨à°‚à°¦à±à°•à±."
]

def normalize(text: str) -> str:
    return normalizer.normalize(text)

def get_sentiment(text: str) -> str:
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        logits = model(**inputs).logits
    label = int(logits.argmax(-1))
    return {0: "ðŸ˜”", 1: "ðŸ˜", 2: "ðŸ˜Š"}[label]

def get_compliment() -> str:
    return random.choice(COMPLIMENTS)

def get_rating(text: str) -> float:
    """
    Dummy rule-based rating for MVP:
    (length + unique words + positive sentiment) â†’ 1-5 stars
    """
    text = normalize(text)
    length = min(len(text) / 200, 1.0)
    unique = min(len(set(text)) / 100, 1.0)
    sentiment = 1.0 if get_sentiment(text) == "ðŸ˜Š" else 0.5
    score = (length + unique + sentiment) / 3 * 5
    return round(score, 1)

def load_stories():
    try:
        with open("stories.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return []

def save_stories(stories):
    with open("stories.json", "w", encoding="utf-8") as f:
        json.dump(stories, f, ensure_ascii=False, indent=2)
